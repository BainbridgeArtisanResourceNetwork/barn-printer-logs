{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Printer Log Usage\n",
    "This notebook is just an automated way to report on 3D printer usage. It expects the latest data file to be downloaded locally, whatever the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Environment\n",
    "I usually run these kind of notebooks in isolated virtual environments, This next bit of code check to make sure that the Python kernel is in the location associated with that virtual environment (only you can tell.) and the Python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "print(\"cwd:{}\".format(os.getcwd()))\n",
    "print(\"exe:{}\".format(sys.executable))\n",
    "print(\"ver:{}\".format(sys.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Imports\n",
    "I'll put all of the libraries/packages that we use in the notebook in this section, so you determine quickly whether there's anything missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "# let seaborn overwrite standard plt\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prep the Data\n",
    "Npw we need to load and prepare the data for downstream usage. We'll check it here and apply what's needed. I'd expect this area to evolve as data grows in complexity or requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(\"..\",\"data\")\n",
    "files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith(\".tsv\")]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(os.path.join(DATA_DIR, files[0]), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of loaded data is {}\".format(df_raw.shape))\n",
    "print(df_raw.columns)\n",
    "print(df_raw.index)\n",
    "# columns can have whitespace around them - kill it\n",
    "df_raw.columns = df_raw.columns.str.strip()\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not need all of these columns at present, so let's extract the ones we do need as a separate copy that we can manipulate as needed. Let's recreate the duration value in hours to be sure its correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_printers = df_raw[['date','name','filament','days','hours','mins']].copy()\n",
    "# one way to do this by using dataframe-specific operations, pretty straight-forward\n",
    "df_printers['usage_hrs'] =  round(\n",
    "    (df_printers['days'] * 24) + (df_printers['hours']) + (df_printers['mins'] / 60),1)\n",
    "\"\"\"\n",
    "# another way is to apply a function across all rows, generating a list which makes a new column\n",
    "# it's probalby more 'Pythonic' but is certainly less clear. this also demonstrated how properly-defined\n",
    "# index names can be used directly as with a dictionary.\n",
    "df_printers['test'] = df_printers.apply(\n",
    "        lambda row: round((row.days * 24) + (row.hours) + (row.mins/60),1), axis = 1) \n",
    "\"\"\"\n",
    "df_printers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any of the rows have no values in the columns we care about, we can detect and filter them out. We could inspect and determine whether there are specific columns to check or we can just filter them all out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_printers[df_printers['filament'].isnull()])\n",
    "print(df_printers[df_printers['usage_hrs'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can either drop just the subset of nulls or we can drop any of them where there is a null column value\n",
    "#df_printers = df_printers.dropna(subset=['filament','usage_hrs'])\n",
    "df_printers = df_printers.dropna()\n",
    "df_printers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's canonicalize names, and make sure dates are actually Date objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dates are easy enough\n",
    "df_printers['date'] = pd.to_datetime(df_printers['date'], format='%Y-%m-%d')\n",
    "\n",
    "#printer names - let's just make them consistent for now\n",
    "df_printers['name'] = pd.Series(df_printers['name']).str.capitalize()\n",
    "\n",
    "df_printers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting\n",
    "We do not have a lot of data to work with here, but we do date, printer name, and usage, so that's what we can summarize.\n",
    "Specifically we want to calculate:\n",
    "* overall usage in terms of filament and time\n",
    "* individual printer usage in terms of filament and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_date = df_printers.groupby(['date'])['filament','usage_hrs'].sum()\n",
    "df_by_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_name = df_printers.groupby(['name'])['filament','usage_hrs'].sum()\n",
    "df_by_name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(figsize=(15,7))\n",
    "#ax.xaxis_date()\n",
    "#ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "axes = df_by_date.plot.bar(subplots=True,rot=30)\n",
    "axes[0].legend(loc=2)\n",
    "axes[1].legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_name.plot.bar(subplots=True,rot=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_date.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
